Selecting Critical Patterns Based on Local Geometrical and Statistical Information
Yuhua Li and Liam Maguire
Abstract? Pattern selection methods have been traditionally developed with a dependency on a specific classifier. In contrast this paper presents a method that selects critical patterns deemed to carry essential information applicable to train those types of classifiers which require spatial information of the training dataset. Critical patterns include those edge patterns that define the boundary and those border patterns that separate classes. The proposed method selects patterns from a new perspective, primarily based on their location in input space. It determines class edge patterns with the assistance of approximated tangent hyperplane of a class surface. It also identifies border patterns between classes using local probability. The proposed method is evaluated on benchmark problems using popular classifiers including multilayer perceptrons, radial basis functions, support vector machines and nearest neighbors. The proposed approach is also compared with four state-of-the-art approaches and it is shown to provide similar but more consistent accuracy from a reduced data set. Experimental results demonstrate that it selects patterns sufficient to represent class boundary and to preserve the decision surface.
Index Terms?pattern selection, data reduction, border pattern, edge pattern.
xxxx-xxxx/0x/$xx.00 ? 200x IEEE
????????????????
* The authors are with the School of Computing and Intelligent Systems, University of Ulster, Londonderry, BT48 7JL, United Kingdom, E-mail: y.li@ulster.ac.uk, l.maguire@ulster.ac.uk.

Manuscript received (insert date of submission if desired).
??????????   ?   ??????????
1	INTRODUCTION
G
IVEN a set of data Do with N patterns, the aim of pattern selection is to select a subset Ds with Ns patterns that can sufficiently represent the original data in terms of some specified criteria (e.g., generalization accuracy), usually . The classifier designed using the subset Ds is expected to maintain generalization performance of the classifier designed on the original entire dataset Do. The benefit of pattern selection is thus obvious when an excessively large dataset is provided for classifier design. For instance-based classifiers such as kNN, storing all instances consumes a lot of memory and computation resource. Therefore a smaller instance dataset requires less memory and results in faster application of the classifier. For other classifiers such as neural networks, a smaller training dataset usually leads to faster training. Moreover training of neural networks often needs to be conducted many times in order to find optimal parameters that control the classifier?s learning and generalization performance. This can translate into using an even smaller training dataset to speed up the training process further. For some classifiers (such as support vector machines which are memory intensive) pattern selection is a necessary process for the large dataset in order to train the classifier on a standard computer [61]. One additional benefit of pattern selection is that it may achieve better classification accuracy due to the removal of noise patterns.
   There are a vast number of pattern selection methods that have been developed for different classifiers and applications [32], [38], [39]. For example, great efforts have been put on selecting patterns for instance-based classifiers (see, e.g., [13], [29], [34], [41], [46], [55], [56], [57], [71]) and support vector machines (see, e.g., [28], [62]). These methods can be categorized using a range of different taxonomies [31]. However, in this work the authors prefer to describe pattern selection methods from two perspectives: geometrical and statistical. Those methods in the geometrical category consider the location of a pattern in the input space; patterns with certain geometrical properties are then selected. Exemplar geometry-based methods include those using hit miss networks [40] and voronoi cells [4]. Geometrical information is used in pattern selection as it is the geometry that counts most to classification accuracy [30]. In contrast methods in the statistical category analyze statistical information of a pattern; a pattern is then selected if it can statistically represent a group of patterns around it [16]. A number of comprehensive review papers are available in the literature for further background in this area [8], [19], [31], [70].
   It is well known that patterns have different levels of usefulness to the training of classifiers [11], [21]. Generally patterns can be described according to their location in the input space. For the convenience of visualization, Fig. 1 uses two-dimensional input space to demonstrate the relationship between patterns of a two-class problem. Patterns can be differentiated using three terms: overlapping, edge and interior patterns. Overlapping patterns are those from different classes mixing with each other because of common features, locations etc. Edge patterns sit on the extremes of a class region, i.e., near free pattern space [63], [70]. Edge patterns define how far a class extends. Interior patterns are within the region formed by edge and overlapping patterns. Note, that if there is no overlapping between classes, then only edge and interior patterns are needed to describe pattern relationships.
   Overlapping patterns introduce considerable difficulty for the learning process and are the primary source of classification error. For this reason, learning algorithms are designed to minimize this kind of error. In this sense overlapping patterns carry more information for classifier training [18]. The decision surface of a well learned classifier passes through the region of overlapping patterns. Thus, most methods focus on selecting patterns from this overlapping region [6]. In this work the authors refer the patterns selected from the overlapping region as border patterns. 
   

   
Fig. 1.	Definition of different types of patterns in an input space.
   If a classifier is trained using the well selected border patterns only, the decision surface can be well learned within the overlapping region. However the decision surface outside of the overlapping region will be arbitrarily formed because there is no pattern to drive the classifier to form the expected decision surface [33], [36]. Moreover for some applications, border patterns alone are not sufficient to carry all the essential information to represent the original data in order to train a classifier with good generalization accuracy. One particular example is novelty detection [5], [42], [60] where the extent of known classes in the input space must be preserved in order to eliminate the mix of patterns from novel classes (a class not seen by the classifier during training). Therefore edge patterns must be included in the training set. However selection of edge patterns is rarely studied.
   This paper presents a training pattern selection method from a new perspective by considering local geometrical and statistical information. Both border and edge patterns are retained in the selected data set. It is intuitive that edge patterns sit astride the surface that separates pattern-occupied region and free region (a free region is a region that no pattern falls within). Based on this intuitive observation, edge patterns are detected by constructing a plane (or hyperplane) tangent to the class surface at a pattern.
   The remainder of the paper is organized as follows. The next section first provides an intuitive discussion on pattern locations in feature space, it then presents the pattern selection method using local geometrical and statistical information. Section 3 reports experimental results on benchmark classification problems and popular classifiers including multilayer perceptrons, radial basis functions, support vector machines and nearest neighbor. Section 4 discusses the potential applications to construction of novelty detection classifiers and similarity-based classifiers, it concludes the paper that the proposed method provides consistent performance on the classifiers considered.
2	THE BORDER-EDGE PATTERN SELECTION METHOD
2.1 Edge Patterns
Consider patterns of a class scattered in input space and that occupy some limited region. This region can be a continuous region that is continuously occupied by data patterns, holed region, or some unconnected sub regions. At the extreme of this region, an imaginary surface can be formed so that all patterns are enclosed within the surface. The surface is so tight that it passes through all extreme external patterns of the class. On a point of the surface, a plane/hyperplane is drawn at a tangent to the surface. The cross point between the class surface and its tangent plane is termed as the edge pattern. The local geometrical shape of the class surface can be convex or concave as shown in Fig. 2. If the surface is convex, all nearest neighbors of the edge pattern will sit on one side of the tangent plane as in Fig. 2(a). If the surface is concave, most of the nearest neighbors of the edge pattern will sit on one side of the tangent plane with a small ratio on the other side as in Fig. 2(b) and (c). The ratio of the majority will depend on the curvature of the surface.
   

Fig. 2.	Illustration of three different curvature shapes of class surface. Solid curved line indicates class surface. Straight line indicates tangent plane. Dashed curved line indicates the region formed by nearest neighbors of an edge point. Solid dots represent edge patterns, and circled dots for interior patterns.
2.2	The Identification of Edge Patterns
The previous section illustrated the relationship of an edge pattern with its nearest neighbors. An edge pattern  will have all or most of its nearest neighbors sitting on one side of the tangent plane passing through . Based on this observation, the task of edge pattern identification becomes the construction of a tangent plane at . Indeed if the tangent plane is defined, its normal vector is then known. The relative locations of nearest neighbors of  with respect to the tangent plane can be determined using the normal vector of the tangent plane. The fact is that the tangent plane is not defined, nor its construction straightforward. An alternative approach is proposed by the authors to solve this problem, i.e. to derive the normal vector for a potential tangent plane using the given data.
2.2.1	Data Surface vs. Density Gradient
The alternative method for construction of the normal vector is derived with an approximation. Assume the data distribution is subject to a density function p(x). It is well known that the gradient of p(x) is normal to the surface of the data distribution.  We now consider how to estimate the gradient of the density function. Using the technique in [23] (p 534), we estimate the density gradient on the basis of given training data points. Suppose the dimensionality of the data is d, for a point x, a hyper sphere with radius r centered at x is constructed, . The sphere has a volume of ? and contains k-nearest neighbors . The expected vector of y in  is estimated as follows [23]:
		(1)
where
		(2)
is the coverage of L(x), and  gives the conditional density function of y given L(x).
Expanding p(y) around x by first order Taylor series 	.	(3)
For a region with radius of , the volume is 
		(4)
and satisfies
	,	(5)
where  is the gamma function.
Substituting (2) and (3) into (1) and using formula (5),
		(6)
Thus 
		(7)
For a given point x, the local mean  can be estimated by the mean of its k-nearest neighbors . Thus the gradient vector  at point x is estimated as
		(8)
where  is a scalar. As only gradient direction is required to give the orientation of the hyperplane at x, we are not concerned with its actual value, i.e., the value of ? is not relevant.
   The above equation shows that the direction of gradient vector at point x points to the same direction of means of its k-nearest neighbors. In , it tends to have more points from the higher density side than from the lower density side. If a point lies on the surface of the dataset, then all or most of the points come from the dense/occupied side. Therefore the task becomes the determination of the distribution of nearest neighbors which is discussed in the following sub-section.
2.2.2	Implementation
For a given pattern  (i=1,2,?N), find its k nearest neighbors (NN) (Euclidean distance is used in this paper). Form a vector from  to each of its k NNs:
		(9)
In order to approximate the normal vector of the tangent plane at , each  is normalized to give a unit vector . The normal vector  of the tangent plane at  is approximated as: 
		(10)
   The formation of the normal vector is illustrated in Fig. 3. The normalization of  is useful as only the vector directions should be used in the derivation of normal vector , it makes all s contribute equally to  without being biased by different magnitudes of s. The normalization indeed is not clear from (8), but it is introduced by modifying the calculation during implementation through intuitive consideration. It is less theoretically rigorous, however an intuitive consideration can justify the normalization process. This is because edge patterns sit on the exterior of the data distribution, where patterns may be noise and sparse. For example, for k=5, suppose one of the 5 nearest neighbours is an extreme pattern due to noise; so the vector formed very likely has the largest magnitude among the 5 nearest neighbours. When using the equation directly, the extreme pattern will drive the direction of the gradient vector from the actual direction towards the extreme pattern. On the other hand, if all the nearest neighbours are noise free, the majority of them would come from the side with high density, normalization would still make the gradient point to the dense side because the majority of neighbours point to the same side.


   
Fig. 3.	Derivation of normal vector  of the tangent plane for a given pattern . Short solid arrows are the normalized vectors from  to its k nearest neighbors.
   After obtaining the normal vector of the tangent plane, the relative location of a neighbor pattern  of the pattern  can be readily decided. If the angle between  and  is within 0 and , then the pattern  is on the positive side of the tangent plane. Equivalently we only need to check the sign of the dot product:
		(11)
If , then pattern  is on the positive side of the tangent plane. If , it is on the tangent plane. Based on the observation in Section 2.1, an edge pattern has all or most of its nearest neighbors sitting on one side of the tangent plane. Thus an edge pattern can be determined by counting the number of neighbors with , denoted as:
		(12)
   For an edge pattern on a convex surface as in Fig. 2(a), all its nearest neighbors are on one side of the tangent plane, so . For an edge pattern on a concave surface, most of its nearest neighbors are on one side of the tangent plane. A threshold, 1-?, must be applied to , where ? is a non-negative but small parameter. If , then the pattern is labeled as an edge pattern. The algorithm for the determination of whether or not a pattern is an edge pattern is summarized in Fig. 4. The procedure when presented with the entire dataset can select all the edge patterns.

// i: index for patterns in the dataset
// j: index for patterns in kNN
For a given pattern 
    find kNNs of  from its class
    for j=1, 2, ?, k
       draw a vector  from  to its jth nearest neighbor
       normalize  to unit vector 
    add up all  to approximate normal vector: 
    for j=1, 2, ?, k
       calculate dot product 
       if 
           increase counter l by one
    find the ratio of kNNs with : 
    if 
       select  as an edge pattern
end

Fig. 4.	Edge patterns selection algorithm
2.3 Border Pattern Determination
The previous section presented the algorithm for selecting edge patterns. These selected edge patterns are sufficient for representing the original dataset if all classes in the dataset are separated. However classes in a real application are very likely to overlap. Thus additional selection work is needed in order to address overlapping cases. It has been well understood that the optimal decision surface passes through the region of overlapping patterns [43], [50], [58], [68]. Many methods have been developed to select patterns for these overlapping classes. Such methods focus on selecting those patterns that preserve the decision surface. In this study, the authors employ a strategy similar to Choi?s method [18] to select these patterns, due to its effectiveness and simplicity of implementation. We improve Choi?s method by enabling it to deal with overlapping as well non-overlapping classes. While Choi?s or similar methods deal with overlapping classes well, however, our investigation shows that it may fail to select patterns if all the classes are well separated. This shortcoming will be remedied in the proposed algorithm by combining edge pattern selection.
   Overlapping patterns present conflicting information to the learning process of a classifier and thus increase the learning difficulty. Therefore the proposed strategy of selecting methods is to remove class overlap in the dataset. The removal/retention is based on Bayes posterior probability of a pattern belonging to a class, P(?|x), where ? is the class and x is the pattern. For a dataset with C classes, estimate . Find .  If  does not occur in the same class as that x actually belongs to, the pattern is removed from the dataset and the algorithm moves to the next pattern. Otherwise further process is evoked to decide whether or not the pattern is selected. Find the number nc of classes that . If  and  (where  controls the degree of pattern removal), then the pattern is close to Bayes decision surface. Such a pattern is selected and termed as border pattern.
2.4 The Overall Algorithm for Critical Pattern Selection
After discussing the selection algorithms for edge in Section 2.2 and border patterns in Section 2.3, it is straightforward to formulate critical pattern selection by integrating individual algorithms. For a given pattern, it is first detected whether or not it is a border pattern. If it satisfies  and  , the pattern is selected as a border pattern and the algorithm moves to the next pattern. If not, the following process is executed: if , the pattern is processed with the edge pattern selection method presented in Section 2.2. The overall algorithm is summarized in Fig. 5 which employs kNN to calculate P(?|x). In addition, the algorithm includes a pre-processing step to check if all patterns in the whole dataset are unique. Duplicated patterns are removed, as they are redundant if a dataset needs reduction.
   The proposed method enhances existing approaches such as Choi?s method [18] with an additional feature of edge pattern selection. If all the classes are well separated, then we always have one class with  for an . This means Choi?s or similar methods will either select all patterns (with ?=0.5) or none (? with other values). Similarly, the state of the art method reported in [62] is effective to select patterns from overlapping classes, however it cannot make any selection if the classes are well separated because its selection criterion of Neighbors_Entropy>0 cannot be satisfied. In contrast, the proposed method automatically switches to edge pattern selection for well separated classes. Therefore it ensures that critical patterns are selected.
   
  // N: number of patterns in the dataset
  // C: number of classes in the dataset
  // i: index for patterns in the whole dataset
  // j: index for patterns in kNN
  // c: index for classes
  // pc: counter for the number of nearest neighbors from class c
  pre-process the dataset to ensure no repetitive patterns
  for i=1 to N
     get class lable  of 
     find kNNs for 
     for j=1 to k
         get the class label for jth nearest neighbor 
         If , increase pc by 1
     for c=1 to C 
         
     find the number nc of classes contained in current kNN
     assign class lable  to based on 
     if 
    if  and 
       select  as a border pattern
    else if 
       call edge pattern selection algorithm
  end
  
Fig. 5.	Critical patterns selection algorithm.
2.5	Parameter Effects
There are four parameters in the proposed method: 
kb  -	the number of nearest neighbors for computing border patterns, 
? ? 	ratio used for deciding border patterns
ke - 	the number of nearest neighbors for computing edge patterns, 
? -  	ratio used for deciding edge patterns.
These four parameters can be divided into two groups: one group with two parameters (kb and ?) for computing border patterns and another group with two parameters (ke and ?) for computing edge patterns. These two parameter groups play a different role in the algorithm, the values of each group parameters can be determined separately. 
   For computing border patterns, kb controls the smoothness of separation between classes, and ? controls the thickness of the band of border patterns. A larger value of kb results in a smother separation between classes, whereas a smaller kb results in a skewer separation. There is a considerable amount of literature about effective selection of kb (e.g., [25], [26], [27], [62]). It is well known that the probability of error of k-NN asymptotically approaches the Bayes error if it satisfies the condition:  and  as  [23]. Based on this condition, we set . 
   The use of ? is useful to classifier design, especially for neural classifiers which require sufficient number of patterns to make reasonable convergence speed of learning. If more patterns are required around the decision surface, a larger value of ? can increase the band thickness of border patterns. Moreover our experience shows that ? can be selected from the range of [0.1, 0.3].
   On the other hand, ke and ? together control the selection of edge patterns. ke handles the localness of nearest neighbors of the current concerned pattern. The determination of ke is very similar to the choosing neighborhood in manifold learning [65]. Automatic choosing neighborhood for manifold learning is a very challenging task and it is still an open research topic [44], [54], [59], [69]. Our case is even more difficult, because in manifold learning datasets are assumed to be densely sampled and lie on or close to a smooth low dimensional manifold [37]. In our case, we are dealing with volumetric/spatial data which may have a sparse distribution around data surface. Here we provide some heuristic insight to the chose of ke. A small ke will constrain nearest neighbors in a very local region around . A large ke will make nearest neighbors span a large space around. Thus a reasonable value for ke should be used in order to reflect the curvature of the class surface. Analogue to the determination of kb, we set the value of ke increasing with the size of dataset but use a bigger value than kb, i.e. . The use of different logarithms for kb and ke is based on the following consideration. On the one hand, both k values satisfy the condition:  and  as . On the other hand, we used smaller value for kb so that the boundary patterns between classes were not drastically smoothed out, and a bigger value for ke to achieve an accurate estimation of local gradient. Moreover, it seems more reasonable to choose ke depending on the training sample size of a particular class. However our experience (see Section 3.4) shows that selection rate and classification accuracy are relatively insensitive to the value of ke if ke is greater than a certain value. Thus we will use a same value of  for all classes in a dataset to simplify the parameter setting.
   The parameter ? has dual effects: it controls the thickness of edge patterns as well as the curvature of the concave class surface. For a class with a local convex surface around , ? only controls the thickness of the shell of edge patterns, i.e., thickness increases with ?. If a local class surface is concave, ? then also equips the selection algorithm with the ability to deal with local curvature of the class surface. Fig. 6 illustrates two class surfaces with different curvatures. The larger the curvature, the larger the proportion of nearest neighbors in the region between the class surface and tangent plane. Thus a large ? relates a bigger curvature surface. Empirically ? can be chosen from the range of [0, 0.1].
   
   
Fig. 6.	Illustration of the distribution of nearest neighbors (NNs) of an edge pattern  with respect to class surface and its tangent plane. The curvature of class surface in (a) is bigger than that of (b), so (a) has more proportion of NNs between class surface and tangent plane.
3 EXPERIMENTS
Extensive experiments have been carried out to evaluate the proposed pattern selection method against four state-of-the-art pattern selection methods using four types of classifiers on 19 datasets. 
3.1	Experimental Setup
3.1.1	Selection Methods and Classifiers
Four representative pattern selection methods were selected in this study: iterative hit miss networks with editing (HMNEI) [40], fast nearest neighbor condensation rule 1 (FCNN1) [4], decremental reduction optimization procedure 3 (DROP3) [70] and neighborhood property-based pattern selection (NPPS) [62].  The selection of data reduction algorithms was based on their representativeness and popularity. DROP algorithms are well known and established techniques. HMN, FCNN and NPPS represent state-of-the-art data reduction algorithms, they reduce data through fundamentally different mechanism and exhibit excellent performance in either maintaining classification accuracy, reduction rate or execution speed. The proposed border edge pattern selection method is denoted as BEPS. The programs for HMNEI and DROP3 were provided by their originators, respectively. We coded FCNN1 and NPPS in Matlab in this study as FCNN1 and NPPS programs were not publicly available elsewhere. 
   The proposed pattern selection method is not developed for a specific classifier but rather it attempts to select critical patterns that preserve information sufficient for a group of classifiers whose generalization accuracy depends mainly on learning from border and edge patterns. Four popular types of classifiers were selected: multilayer perceptrons (MLP) [52], radial basis functions (RBF) [9], support vector machines (SVM) [66] and k-nearest neighbor rule (kNN). These four types of classifiers were considered to be the most representative ones for the evaluation of the proposed algorithm due to their characteristics as discussed as follows. It is known that the decision surface of a MLP is unbounded [36], so an MLP forms an arbitrary decision surface in regions where there are no training patterns. Thus a pattern selection method should select patterns that well represent the boundary formed by the original data in order to achieve a successful training of a MLP. Although a RBF is capable of generating a bounded decision surface, it requires the selected patterns to represent regions of the original data. Therefore edge patterns are essential to the generalization performance of both MLPs and RBFs and their inclusion in these experiments will evaluate the effectiveness of the edge pattern selection capability of the proposed method. Moreover, a SVM is used to evaluate whether the proposed method properly select patterns that preserve the decision surface, because it learns the decision surface by using only the vectors/patterns around the decision surface [10]. For some applications of SVM such as data domain description, edge pattern must be retained [64]. In addition, kNN is considered in this experiment because it is the main (often the only) classifier used in other pattern selection studies, e.g., [4], [40], [70]. All classifiers are implemented in Matlab on a PC: MLP and RBF using the Neural Networks Toolbox, SVM using Graz?s SVM toolbox for Matlab with error-correcting output codes for multi-class classification [1] and kNN our own program. In the experiments, MLPs used one hidden layer, SVM  used Gaussian kernel, and kNN used the same value k=1 and Euclidean distance as in other studies.
3.1.2	Datasets
Nineteen datasets were used in this experiment to demonstrate the effectiveness of the proposed method for a variety of classification tasks involving different data characteristics. They are grouped into COMMON datasets and BENCHMARK datasets. 
   Seven COMMON datasets were chosen from commonly used classification problems. ripley is from Oxford (http://www.stats.ox.ac.uk/pub/PRNN/), hart is from [29] and widely used in pattern selection research, but we will modify it in the following experiemnts to generate three classes with overlapping between classes. phoneme is from the European ESPRIT 5516 project [2], svmguide1 from LIBSVM [15], a5a from the UCI machine learning repository [45], ijcnn1 from the first problem of IJCNN2001 challenge [49]. svmguide1, a5a and ijcnn1 are taken from LIBSVM?s data collection [15]. The datasets are chosen such that the size of the dataset varies from thousands to tens of thousands and the number of features range from a few to over one hundred. The authors believe that these selected datasets are representative of a range of problems in terms of scale and as a result give a fair evaluation to the pattern selection method presented. BENCHMARK datasets are those that were ?standardised? classification problems originated in [51] and later widely used by many others (e.g., [12], [41]), but titanic is excluded from our experiment as it contains only 24 unique patterns which render selection meaningless. In addition, like other researchers using these same data sets having discrete/categorical variables (e.g, a5a and flare_solar), we handled them as ordinary numerical variables in distance calculation. Though this treatment is not perfect, it is acceptable if we are not concerned about the physical meanings of individual variables.
   In the interests of repeatability, we used the same partitions of training and test data as they were in the repository. Specifically, each of the COMMON datasets has a training and test set, and each of the BENCHMARK datasets has 100 partitions except splice and image having 20 partitions. Therefore, the test accuracy is measured using hold-out method for COMMON datasets and 100 (or 20) fold cross validation for BENCHMARK datasets. All the datasets are shown in Table 1.
   Pattern selection methods were applied to each training data to select patterns. MLP, RBF and SVM classifiers were trained on the original unselected training set Do and selected sub-sets, respectively. This evaluation experiment procedure is computationally extremely expensive considering the combination of partitions (100 partitions for most BENCHMARK datasets), selection methods, different types of classifiers and different parameter settings for classifiers. In total, it involved about 106 classifiers of trained MLP, RBF and SVM. This is much more computationally expensive than reported pattern selection studies as most of them consider only kNN classifier which does not involve training (k=1 has usually been used).
   In all the following experiments, we use fixed parameters for BEPS based on the discussion in Section 2.5: , ,  and .
TABLE 1
SUMMARY OF DATASETS
Dataset
Variables
Training patterns
Test patterns
XOR
2
1000
1000
ripley
2
250
1000
hart
2
4998
4998
phoneme
5
2161
3243
svmguide1
4
3089
4000
a5a
123
6414
26147
ijcnn1
22
49990
91701
banana
2
400
4900
breast_cancer
9
200
77
diabetis 
8
468
300
flare_solar
9
666
400
german
20
700
300
heart
13
170
100
image
18
1300
1010
ringnorm
20
400
7000
splice
60
1000
2175
thyroid
5
140
75
twonorm
20
400
7000
waveform
21
400
4600
   
3.2	BEPS Behaviors
We first illustrate the behavior of BEPS by showing relative locations of the selected patterns. Three commonly used synthetic problems are used to illustrate the behavior of BEPS, XOR, Ripley?s and Hart?s classification problems. The four clusters of the two class XOR data was generated using normal distribution , where ? is the mean vector and  is the variance. The four clusters of the XOR data in this study have a variance of 0.52, class one has mean vectors of (1, 1) and (-1, -1), class two has mean vectors of (-1, 1) and (1, -1). 1000 patterns were generated for the training data, 1000 for test. For Ripley?s data, the training set contains 250 patterns, and test set 1000 patterns. The Hart?s original problem is a two-class classification with patterns uniformly distributed in a rectangle, but we split one of its two classes into two to create a problem with three classes. We also added overlap of 0.04 along left vertical border between classes. 4998 patterns were generated for the training data, 4998 for test, with equal number of patterns in each of the three classes.
   The proposed method was applied to the training sets of the three synthetic problems. Fig. 7(a) shows the original data and the selected data for XOR problem, and Fig. 7(b) and 7(c) show the original training data and the selected data for Ripley?s and Hart?s problems, respectively.  It is clear from the figure that the selected patterns preserve patterns around border and occupying limits of data distribution.






           (a)











          (b)











           (c)







Fig. 7.	Original training set and selected sub-set, dot for class 1, cross for class 2, and plus for class 3. Points with circles are the selected edge patterns, points with diamonds are the selected border patterns. (a) XOR data; (b) Ripley?s data; (c) Hart?s data (only show 1500 patterns for clarity).
   Individual classifiers were trained using the original whole training set and selected sub-sets, respectively. Fig. 8 and Fig. 9 show the comparison of decision surfaces of classifiers derived from the whole datasets and selected subsets for XOR and Ripley?s datasets, respectively. It is obvious that, for each type of classifier, the decision surface of the classifier trained on selected subsets matches very well with that using the original whole dataset.
   The results on synthetic problems illustrate that the proposed method is able to select critical patterns that maintain information for classifier training. The following sections will evaluate the proposed method using all the datasets.


Fig. 8.	Comparison of decision surfaces of SVM, MLP and RBF classifiers trained from the whole original data and the selected data for XOR problem. Solid line for decision surface of classifier trained using selected data, dashed line for original data.

Fig. 9.	Comparison of decision surfaces of classifiers trained from the whole original data and the selected data for Ripley?s problem.  A Solid line is used for the decision surface of the classifier trained using selected data, a dashed line for the original data.
   
3.3	Selection Rates
Selection rates of the proposed method and the four state-of-the-art methods are listed in Table 2. The selection rates of BEPS on COMMON datasets are relatively small except a5a dataset selected 82.80%. This high percentage is due to the nature of a5a data as each feature of the a5a data takes two values: 0 and 1, so all patterns locate on the edge of a unit hyper-cube. These edge patterns are selected using the proposed method, but there is still some reduction coming from overlapping patterns. On BENCHMARK datasets, BEPS selects high percentage of patterns from the datasets. While high selection rate may be against the proposed method, BEPS does show a desirable feature of selection methods, i.e., high percentage of patterns should be selected if a dataset is small in order to achieve a good classifier training. Recall the data summary listed in Table 1, these BENCHMARK datasets have a small size ranging from 140 to 1300 patterns. Thus keeping more patterns is reasonable. 
   In order to assess the overall selection performance, some statistics are calculated over all dataset for each selection method and the results are listed in the last four rows of Table 2. Wilcoxon signed rank test [20] is used to perform  a  paired,  two-sided signed rank test of the null
TABLE 2
SELECTION RATES (%)

BEPS
HMNEI
DROP3
NPPS
FCNN1
XOR
12.00
24.10
33.50
17.60
14.80
ripley
26.80
22.80
35.20
32.40
28.40
hart
14.71  
58.14  
33.51  
17.03   
9.94
phoneme
22.54
44.42
32.30
39.52
30.68
svmguide1
9.81
50.96
34.67
15.18
12.59
a5a
82.80
26.94
24.21
45.74
37.12
ijcnn1
19.78
n/a
n/a
12.44
9.74
banana
15.89
42.08
31.72
31.23
28.77
breast_cancer
51.55
27.2
27.07
56.77
55.34
diabetis
46.37
26.85
26.51
50.16
52.74
flare_solar
62.17
7.91
6.2
52.67
52.45
german
68.19
24.49
25.7
57.09
54.2
heart
72.63
22.83
27.88
49.47
44.01
image
31.34
42.67
35.07
19.85
14.18
ringnorm
51.58
17.13
19.35
5.32
29.76
splice
73.31
13.38
20.98
65.33
48.27
thyroid
23.99
41.2
34.24
14.43
15.06
twonorm
96.34
16.62
27.25
27.54
20.64
waveform
88.3
20.14
26.44
35.96
30.74
Average
45.80
29.44
27.88
33.98       
31.02
Median
46.37                            
25.67
27.57
32.40        
29.76
Wilcoxon h

0
1
0
1
Wilcoxon p

0.0707
0.0429
0.0585
0.0141
   
hypothesis that there is no significant difference between BEPS and each of the other methods, against the alternative that there is a significant difference. In this and following tables, significance level of 0.05 is used. For convenience of reading, both hypothesis test result and p-value are listed. h = 1 indicates a rejection of the null hypothesis at the 0.05 significance level and h = 0 indicates a failure to reject the null hypothesis at the 0.05 significance level. Based on these statistics, DROP3 and FCNN1 select fewer patterns than BEPS. However, if the significant level is changed to 0.01, there is no significant difference between BEPS and each of the other methods. Note that ijcnn1 is too big for HMNEI and DROP3 programs, so it is excluded from the calculation of Average, Median and Wilcoxon signed rank test for HMNEI and DROP3.
3.4	Classification Performance
As mentioned before, classification accuracy is measured using hold-out method for COMMON datasets and 100 (or 20) fold cross validation for BENCHMARK datasets. So classification accuracy for a benchmark dataset is reported as the average of test accuracies over test sets of all partitions. We perform two hypothesis tests, paired t-test and Wilcoxon signed rank test, to compare BEPS with other methods. Paired t-test is used for a specific dataset and Wilcoxon signed rank test for the entire group of 19 datasets. Both tests use 0.05 significance levels. In the following tables, a ?+? indicates that BEPS?s average accuracy is significantly higher than the other method at a 0.05 significance level, a ??? indicates that BEPS?s average accuracy is significantly lower than the other method at a 0.05 significance level, and a ?~? indicates that there is no significant difference between BEPS?s average accuracy and the other method at a 0.05 significance level. Classification accuracy is reported for SVM, MLP, RBF and 1NN, respectively.
TABLE 3
CLASSIFICATION ACCURACY FOR SVM 

BEPS
HMNEI
DROP3
NPPS
FCNN1
Do
XOR
96.00
95.90
95.80
95.80
95.30
95.70
ripley
90.10
90.40
90.40
90.40
90.90
90.40
hart
97.24          
97.16
96.36
97.25
96.74
97.26
phoneme
84.64
82.39
83.81
84.30
83.93
85.11
svmguide1
96.70
96.63
96.65
96.78
96.47
97.17
a5a
84.14
83.75
84.18
84.28
84.73
84.49
ijcnn1
98.12
n/a
n/a
98.50
98.62
98.92
banana
89.42
89.41~
87.68+
88.02+
88.63+
89.54-
breast_cancer
76.51
71.60+
70.83+
72.08+
70.47+
74.48+
diabetis
76.72
70.99+
68.54+
73.21+
71.50+
76.30+
flare_solar
66.41
66.63~
63.83+
65.35+
64.77+
67.55-
german
76.75
76.06+
74.38+
76.22+
76.72~
76.58~
heart
85.14
83.56+
82.60+
84.35+
83.03+
83.72+
image
95.14
92.79+
94.57+
75.35+
94.81~
96.92-
ringnorm
80.07
97.90-
98.15-
90.46-
94.54-
98.41-
splice
86.25
79.60+
83.14+
85.34+
87.03-
89.21-
thyroid
94.35
96.47-
96.13-
93.67~
96.87-
95.77-
twonorm
97.66
97.26~
96.96+
97.19+
96.86+
97.34+
waveform
89.95
86.75+
88.22+
89.54+
89.40+
90.12-
Average
87.44                                        
86.40       
86.24
86.21
87.44
88.68
Median
89.42                                   
88.08        
87.95
88.02
89.40        
90.12
Wilcoxon h

1
1
1
0
0
Wilcoxon p

0.0475
0.0249
0.0176
0.3048
0.1164
   
   Table 3 lists the classification accuracy for SVM classifier. Wilcoxon signed rank test shows that classifiers trained using BEPS sub-sets have similar classification accuracy as those of FCNN1 and Do, but better than HMNEI, DROP3 and NPPS at 0.05 significance level. 
   Test accuracy of MLP is reported in Table 4. At 0.05 significance level, BEPS performs equally as HMNEI, NPPS and Do but better than DROP3 and FCNN1. This may be due to the different strategies used in different selection methods. HMNEI tend to select patterns that lead to maximize margin between classes. NPPS selects all patterns within overlapping region. BEPS selects patterns around decision boundary as well as patterns defining extent of input space. Thus HMNEI, NPPS and BEPS select patterns that preserve the information required for MLP training. 
TABLE 4
CLASSIFICATION ACCURACY FOR MLP

BEPS
HMNEI
DROP3
NPPS
FCNN1
Do
XOR
95.70
96.00
95.70
95.60
95.90
95.3
ripley
91.70
88.70
91.10
90.80
91.60
90.1
hart
97.08          
97.06
97.14
97.32
97.62
97.66
phoneme
85.07
81.68
84.12
82.24
84.86
85.63
svmguide1
96.90
96.72
96.90
96.97
96.72
96.75
a5a
83.95
82.93
80.95
83.67
81.45
82.18
ijcnn1
98.29
n/a
n/a
97.82
98.43
98.93
banana
89.10
88.64+
87.22+
88.66+
88.41+
89.32?
breast_cancer
76.64
72.87+
73.26+
76.49~
73.23+
73.90+
diabetis
77.39
76.09+
74.06+
77.42~
75.02+
76.15+
flare_solar
65.59
67.25?
64.27+
65.72~
64.77+
67.28?
german
76.90
75.18+
73.92+
77.24?
72.71+
75.01+
heart
85.99
85.53+
82.93+
86.06~
82.57+
83.06+
image
95.36
92.99+
95.80?
78.02+
93.71+
98.09?
ringnorm
67.16
75.97?
76.54?
71.99?
68.55?
85.83?
splice
86.07
80.04+
82.84+
86.45~
87.37+
90.70+
thyroid
93.68
95.61?
94.64?
92.89+
95.09?
96.01?
twonorm
97.36
97.55~
96.56+
97.20+
96.33+
96.80+
waveform
89.73
85.58+
87.01+
89.36+
87.80+
88.72+
Average
86.82                           
85.36       
85.28
85.89       
85.90       
87.76
Median
89.10                           
85.56        
85.57
86.45       
87.80        
89.32
Wilcoxon h

0
1
0
1
0
Wilcoxon p

0.0777
0.0262
0.1474
0.0442
0.9519
   
   Table 5 lists the test accuracy of RBF classifier. Note that ijcnn1 is not shown in the table as the dataset is too big for the RBF training program we had available. Results of Table 5 show that BEPS outperforms DROP3, NPPS and FCNN1. There is no significant difference between BEPS and HMNEI based on statistical hypothesis test. Moreover, BEPS achieves accuracy performance similar to that of classifiers trained on whole training sets Do. 
   In recent years, an active research topic is the development of pattern selection algorithms for instance-based classifiers such as kNN. Therefore, a set of classification accuracy comparison is conducted for kNN. Same as other studies, we also used k=1 to give a fair comparison. Table 6 shows test accuracies of 1NN using selected subsets of the five algorithms and the whole datasets. BEPS, NPPS and Do give similar level of overall test accuracy. As reported in [40], HMNEI had had the best classification accuracy for 1NN. Results of Table 6 demonstrate that BEPS nearly matches HMNEI in terms of classification performance of 1NN using selected subsets. FCNN1 was shown to have attractive features of maintaining accuracy and excellent learning scaling [4], but these results show that 1NN accuracy of BEPS outperforms that of FCNN1. In addition, DROP3 is a well known pattern selection method, it has been widely used as a benchmark algorithm in developing new algorithms due to its good generalization performance. This experiment also shows that BEPS performs better than DROP3. Although BEPS is not specially developed for 1NN, the results show that it maintains 1NN classification accuracy as that of using whole dataset Do.
TABLE 5
CLASSIFICATION ACCURACY FOR RBF

BEPS
HMNEI
DROP3
NPPS
FCNN1
Do
XOR
95.80
95.40
95.30
77.10
92.40
95.7
ripley
90.80
90.40
90.80
81.20
90.80
90.6
hart
96.04          
96.60
96.34
93.70
92.88
97.12
phoneme
84.30
83.41
84.03
83.66
82.82
85.81
svmguide1
96.30
96.28
96.40
87.63
93.05
96.97
a5a
83.39
83.41
82.90
83.66
83.56
84.36
banana
85.58
88.31?
80.50+
75.96+
66.57+
88.54?
breast_cancer
74.35
72.68+
71.82+
73.56+
71.43+
71.92+
diabetis
73.32
72.83~
67.51+
73.10~
65.22+
71.99+
flare_solar
64.80
65.88?
63.80+
65.09~
61.54+
65.57?
german
75.18
71.88+
70.89+
72.24+
72.55+
74.55+
heart
82.09
70.13+
68.41+
75.44+
64.39+
77.17+
image
94.95
92.50+
94.20~
73.60+
87.84+
96.96?
ringnorm
52.46
58.70?
55.79?
50.54+
67.09?
98.24?
splice
84.83
76.28+
82.10+
84.05+
85.60-
87.79?
thyroid
87.19
87.76~
85.72~
83.57+
81.36+
89.91?
twonorm
96.92
54.39+
55.55+
52.58+
51.38+
96.80~
waveform
88.58
72.52+
73.77+
63.47+
61.92+
88.03+
Average
83.72                                   
79.41       
78.66
75.01
76.24
86.56
Median
85.21
79.85        
81.30
75.70
76.96
88.29
Wilcoxon h

0
1
1
1
0
Wilcoxon p

0.1168
0.0036
0.0004
0.0042
0.1840
   
   The above experiments show that BEPS achieves good classification accuracies for the four types of classifiers on the selected datasets. However BEPS involves parameter setting which may be perceived as its drawback. Therefore, a set of experiments was designed to investigate the effect of parameter setting on classification accuracy. In particular, the effect of parameters (ke and ?) for edge pattern selection is considered in this experiment, as the effect of border pattern parameters has been extensively studied in the literature as discussed in Section 2.5. Fig. 10 shows the effect of parameters (ke and ?) on selection rate and classification accuracy on XOR data. In the experiment, the values of kb and ? are fixed at the same values as that in previous experiments, ke  varies from 1 (a single nearest neighbor) to three times of (i.e., 3x35), ? from 0 to 0.3. 
   
TABLE 6
CLASSIFICATION ACCURACY FOR 1NN

BEPS
HMNEI
DROP3
NPPS
FCNN1
Do
XOR
95.40
95.60
94.70
95.20
92.50
94.10
ripley
87.90
90.20
81.40
88.30
83.50
85.00
hart
96.54          
96.68
95.56
96.70
96.06
96.58
phoneme
82.49
83.26
81.78
83.60
84.00
86.56
svmguide1
93.65
95.72
94.20
95.88
92.88
94.92
a5a
82.14
81.65
77.26
82.20
75.08
78.33
ijcnn1
95.37
n/a
n/a
79.13
94.96
97.39
banana
87.67
88.57?
84.83+
86.39+
84.43+
86.36+
breast_cancer
74.30
69.25+
65.17+
73.74~
63.29+
67.30+
diabetis
73.22
73.52~
67.35+
73.37~
66.55+
69.88+
flare_solar
63.44
64.69?
58.62+
57.24+
58.60+
60.78+
german
74.05
72.85+
68.71+
73.52+
66.66+
70.54+
heart
80.98
81.29?
76.18+
80.65~
72.55+
76.84+
image
93.38
92.72+
93.73~
69.64+
95.38?
96.62?
ringnorm
51.29
65.59?
60.36?
63.29?
80.76?
64.97?
splice
68.78
70.71?
65.27+
67.92+
68.87~
71.15?
thyroid
90.92
93.17?
91.93?
89.95+
93.35?
95.64?
twonorm
94.01
95.90?
91.99+
89.98+
88.59+
93.32+
waveform
85.36
85.40~
82.92+
83.99+
81.19+
84.17+
Average
82.68
83.15       
79.55
80.56       
81.01       
82.66
Median
85.36
84.33         
81.59
82.20       
83.50       
85.00
Wilcoxon h

1
1
0
1
0
Wilcoxon p

0.0475
0.0096
0.0989
0.0218
0.6009
   
   The following observations can be made based on Fig. 10. In general the selection rate increases slightly with increase in ke, but rapidly with increase in ?, as shown in Fig. 10(a). Interestingly, the selection rate is high when ke is a small number (<10 for this data). This is due to gradient direction estimation inaccuracy and unevennesss of data distribution. The data may scattered in the domain with some local areas dense and others sparse or empty, so it may lead to all or most nearest neighbors to locate to one side of the hyperplane if ke is small, and the pattern is falsely assigned to an edge pattern even if it is actually an interior pattern.  Once the value of ke is greater than a certain value (>11 for this case), the selection value is suddenly droped. Fig. 10(b-e) show the effect of ke and ? on classification accuracy. It is obvious that classification accuracy is insensitive over a wide range of ke and ?. This implies that the parameter setting for BEPS is relatively easy which allievates the parameter issue associated with the proposed algorithm.
   
   
    
(a)		   (b)			(c) 			(d)		      (e)

Fig. 10.	Effect of parameters (ke and ?) on selection rate and classification accuracy on XOR data.

   
3.5	Empirical Computational Complexity
All of BEPS and the other four selection algorithms involve the calculation of distances between pairs of patterns within the dataset, so they have quadratic computational complexity without considering speedup strategies. The difference among them is that whether or not one requires iterative calculation. BEPS and NPPS select all patterns through a one-off process over the given data, while the other three algorithms perform a number of iterations to output the final selected subsets. In this sense, BEPS and NPPS should execute faster than the others. However, FCNN1 algorithm exploits triangle inequality [24] to minimize the number of required distance calculation, so it often achieves a very fast selection for large datasets.
TABLE 7
EXECUTION TIME OF SELECTION METHODS (IN SECOND)

beps
hmn
Drop3
npps
fcnn
XOR
0.889
3.557
14.618
0.515
0.796
ripley
0.141
0.967
0.249
0.063
0.094
hart
11.949
39.390
1878.500
7.769
10.125
phoneme
3.057
10.749
126.765
1.872
5.912
svmguide1
6.069
19.391
439.452
3.432
5.148
a5a
127.132
1484.636
3295.516
144.268
103.226
ijcnn1
2259.600
n/a
n/a
2171.255
1094.512
banana
0.220
3.162
1.097
0.116
0.222
breast_cancer
0.092
0.279
0.104
0.051
0.107
diabetis 
0.243
1.294
0.965
0.155
0.410
flare_solar
0.350
3.441
2.659
0.258
1.250
german
0.505
3.609
3.565
0.356
0.977
heart
0.080
0.243
0.103
0.042
0.073
image
1.513
21.172
34.501
0.870
1.192
ringnorm
0.214
4.442
1.132
0.131
0.296
splice
1.224
12.112
11.938
0.853
2.458
thyroid
0.067
0.209
0.085
0.033
0.042
twonorm
0.269
4.721
1.443
0.143
0.209
waveform
0.281
3.503
1.254
0.134
0.256
Average
127.050                           
89.827        
323.000
122.750
64.595
Median
0.350                           
3.583
2.051
0.258        
0.796
Wilcoxon h

1
1
1
0
Wilcoxon p

0.0002
0.0002
0.0019
0.5197
   
   The execution time of individual algorithms on our computer is presented in Table 7. Statistically the overall execution speed of BEPS is faster than HMNEI and DROP3, slower than NPPS and similar to FCNN1. However a detailed examination reveal that FCNN1 has the fastest speed when dataset size is large. This confirms the best scaling behavior of FCNN1 as reported in [4]. It should be pointed out that the results of Table 2 were obtained from the programs we had available. Different implementation of the algorithms may result in somewhat different speed. In particular, we did not attempt at all to optimize the BEPS program for faster speed. Therefore Table 7 can only be used as an indication of relative speed of the algorithms. However, this time overhead listed in Table 7 is still very small compared with the time duration required for full training of a classifier. For example, for using grid searching for optimal SVM parameters, a full training of SVM over 100 partitions of banana data took about one hour (3874 seconds exactly) on BEPS selected subsets but about 31 hours (111289 seconds) on the whole training sets of the 100 partitions, on a 3.19 GHz Pentium 4 CPU with 512 MB of RAM. 
   The time complexity of BEPS can be reduced by incorporating an efficient nearest neighbors searching algorithm. There are a large number of algorithms for searching nearest neighbors (e.g., [3], [14], [53]). It is possible to achieve a computation complexity proportional to logarithmic size of data [7], [22]. As the nearest neighbors searching part of BEPS is not intertwined with other parts of the algorithm, an efficient searching algorithm can be readily integrated into BEPS. However, this paper will not exploit this speedup as its focus is on selection and accuracy maintaining performance.
4 DISCUSSION AND CONCLUSIONS
This paper develops a pattern selection method from a new perspective. The method selects patterns representing the data distribution boundary in input space as well as preserving the decision surface. The purpose of this study is to develop a general method that selects critical patterns preserving essential information required for training some types of classifiers to which spatial information is demanded for successful training. It should also be noted that the selected critical patterns include only the edge and border patterns but not those interior patterns. Therefore the probabilistic distributions of the classes are destroyed and consequently the proposed methods are mainly applicable for boundary-based classifiers such as those considered in Section 3 rather than distribution-based ones requiring probability densities. 
   Most existing pattern selection methods select patterns from overlapping regions. Two drawbacks of these existing methods are: they fail to select patterns if there is no overlapping between classes and the selected patterns do not represent the full extent of the data distribution as required for appropriate training of some classifiers. The proposed method overcomes these drawbacks because it selects border patterns from overlapping regions and edge patterns that represent the full extent of a dataset; it automatically switches to edge pattern selection if there is no overlapping between classes. Four state-of-the-art pattern selection methods, nineteen classification problems and four types of popular classifiers were chosen to evaluate the method. The following conclusions are made based on the experiments with respect to the problems and classifiers. The proposed algorithm selects patterns that retain information for classifier learning in terms of classification accuracy. The proposed method achieved the same level of classification accuracies as those trained on the whole datasets for all the four types of classifiers on the 19 classification problems. Furthermore, the proposed approach, demonstrated a more consistent performance in comparison with the four state-of-the-art pattern selection methods.
   The proposed algorithm selects critical patterns containing all edge patterns on the extreme of the data distribution. It has the same objective as those techniques that can deal with data extremes, e.g., spatial depth estimation [17]. Spatial depth quantifies how deep a pattern is located in a data distribution, with deepest the distribution center (usually median) and surface the distribution extremes. In this sense, the proposed selection algorithm is related to finding patterns having low local spatial depth. However the proposed algorithm is intuitively simple, efficient and easy to implement; it carries out selection directly in the input space which may provide a more natural way of data interpretation [67].
   In addition to the main objective of data reduction, the proposed method is expected to have some potential applications, such as novelty detection and dissimilarity-based classification. Novelty detection is a technique that designs a classifier having the ability to exclude a novel event that is not presented during training. It has received interest from applications including the detection of new objects in a video frame [42] or the identification unknown faults in machines [35]. In the design of novelty detector, an essential requirement is that the decision surface of classifier must be closed/bounded in order to detect a novel event. A way to achieve closed decision surface is to generate data enclosing the available training data Do to force the classifier to form closed surface during training [42]. However, to effectively generate such data is still an open research area. The proposed method could be applied to generate data that form an enclosed surface around Do. Another potential application is for dissimilarity-based classification. One important issue in the design of dissimilarity-based classifiers is the choice of a small set of representative prototypes from the entire training set. Although good progress has been made recently, the choice of a representative set is not yet fully investigated [47]. The method reported in this paper is envisaged to provide a systematic approach to prototype selection for dissimilarity-based classifiers, as it selects patterns around extremes of training data. These extreme patterns define the boundary of the data and should be able to provide all dissimilarity information in the entire data. All these potential applications are worthy of further investigation.
ACKNOWLEDGMENT
The authors  would like to thank E. Marchiori and D.R. Wilson for providing us with their pattern selection programs. The authors also thank the anonymous referees for their insightful comments to the improvement in technical contents and paper presentation. This work was supported in part by a grant (Ref: NAL/32568) from the Nuffield Foundation.
REFERENCES
[1] http://www.support-vector.net/software.html.
[2] ftp://ftp.dice.ucl.ac.be/pub/neural-nets/ELENA/databases/REAL/phoneme/.
[3] S. Ajoka, S. Tsuge, M. Shishibori, and K.  Kita, "Fast multidimensional nearest neighbor search algorithm using priority queue," Electrical Engineering in Japan, vol. 164, no. 3, pp. 69-77, 2008.
[4] F. Angiulli, "Fast nearest neighbor condensation for large data sets classification," IEEE Transactions on Knowledge and Data Engineering, vol. 19, no.  11, pp. 1450-1464, 2007.
[5] F. Angiulli, ?Condensed Nearest Neighbor Data Domain Description,? IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.  29, no. 10, pp. 1746-1758, 2007.
[6] R. Barandela, F.J. Ferri, and J.S. Sanchez, ?Decision boundary preserving prototype selection for nearest neighbour classification,? International Journal of Pattern Recognition and Artificial Intelligence, vol. 19, no. 6, pp. 787-806, Sep. 2005.
[7] J.L. Bentley and J.H. Friedman, "Data-structures for range searching," Computing Surveys, vol. 11, no. 4, pp. 397-409, 1979.
[8] J.C. Bezdek and L.I. Kuncheva, ?Nearest prototype classifier designs: An experimental study,? International Journal of Intelligent Systems, vol. 16, no. 12, pp. 1445-1473, Dec. 2001.
[9] D.S. Broomhead and D. Lowe, ?Multivariable function interpolation and adaptive networks,? Complex Systems, vol. 2, pp. 321?335, 1988.
[10] C.J.C. Burges, ?A Tutorial on Support Vector Machines for Pattern Recognition,? Data Mining and Knowledge Discovery, vol. 2, pp. 121?167, 1998.
[11] C. Cachin, ?Pedagogical pattern selection-strategies,? Neural Networks, vol. 7, no. 1, pp. 175-181, 1994.
[12] G. C. Cawley and N. L. C. Talbot, "Efficient leave-one-out cross-validation of kernel Fisher discriminant classifiers", Pattern Recognition, vol. 36, pp 2585-2592, 2003.
[13] V. Cerver and F.J. Ferri, ?Another move toward the minimum consistent subset: a tabu search approach to the condensed nearest neighbor rule,? IEEE Transactions on Systems, Man, and Cybernetics part B: Cybernetics, vol. 31, no. 3, pp. 408-413, 2001.
[14] S.H. Cha and S. N. Srihari, "A fast nearest neighbor search algorithm by filtration," Pattern Recognition, vol.35, no. 2, pp. 515-525, Feb. 2002.
[15] C.C. Chang and C.J. Lin, LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[16] D. Chaudhuri, C.A. Murthy, and B.B. Chaudhuri, ?Finding a subset of representative points in a data set,? IEEE Transactions on Systems, Man and Cybernetics, 24(9): 1416-1424, Sep 1994.
[17] Y.X. Chen, X. Dang, H.X. Peng, and H.L. Bart, "Outlier Detection with the Kernelized Spatial Depth Function," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 288-305, Feb. 2009.
[18] S.H. Choi and P. Rockett, ?The training of neural classifiers with condensed datasets?, IEEE Transactions on Systems Man and Cybernetics Part B-Cybernetics, vol. 32, no. 2, pp. 202-206, Apr. 2002.
[19] B.V. Dasarathy, ?Minimal consistent set (MCS) identification for optimal nearest neighbor decision systems design,? IEEE Transactions on Systems, Man and Cybernetics, vol. 24, no. 3, pp. 511-517, 1994.
[20] J. Demsar, "Statistical comparisons of classifiers over multiple data sets," Journal of Machine Learning Research, vol. 7, pp. 1-30, Jan. 2006.
[21] G.M. Foody, ?The significance of border training patterns in classification by a feedforward neural network using back propagation learning,? International Journal of Remote Sensing, vol. 20, no. 18, pp. 3549-3562, Dec. 1999.
[22] J. H. Friedman, J. L. Bentley, and R. A. Finkel "An algorithm for finding best matches in logarithmic expected time," ACM Transactions on Mathematical Software, Volume 3 ,  Issue 3, pp. 209 - 226, Sept. 1977.
[23] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd Ed. Morgan Kaufmann, Los Altos, CA, 1990.
[24] K. Fukunage, P.M. Narendra, ?A branch and bound algorithm for computing k-nearest neighbors,? IEEE Transactions on Computers, vol. 24, no. 7, pp. 750- 753, 1975.
[25] K Fukunaga and L.D, Hostetle, ?Optimization of k nearest-neighbor density estimates,? IEEE Transactions on Information Theory, vol. 19, no. 3, pp. 320-326, 1973.
[26] A.K. Ghosh, ?On nearest neighbor classification using adaptive choice of k,? Journal of Computational and Graphical Statistics, vol. 16, no. 2, pp. 482-502, Jun. 2007.
[27] A.K. Ghosh, ?On optimum choice of k in nearest neighbour classification,? Computational Statistical & Data Analysis, vol. 50, pp. 3113-3123, 2006.
[28] G. Guo and J.S. Zhang, ?Reducing examples to accelerate support vector regression,? Pattern Recognition Lectters, vol. 28, no. 16, pp. 2173-2183, 2007.
[29] P. Hart, "The condensed nearest neighbor rule," IEEE Transactions on Information Theory, vol. 14, no. 3, pp.515 - 516, 1968.
[30] T.K. Ho and M. Basu, ?Complexity measures of supervised classification problems,? IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 3, pp. 289-300, Mar. 2002.
[31] S.W. Kim and B.J. Oommen, ?A Brief Taxonomy and Ranking of Creative Prototype Reduction Schemes,? Pattern Analysis and Applications Journal, vol.  6, no. 3, pp. 232-244, 2004.
[32] S.W. Kim and B. J. Oommen, ?Enhancing prototype reduction schemes with LVQ3-type algorithms,? Pattern Recognition, vol. 36, no. 5, pp. 1083-1093, 2003.
[33] M.A. Kramer and J.A. Leonard, ?Diagnosis Using Backpropagation Neural Networks - Analysis and Criticism,? Computers & Chemical Engineering, vol. 14, no. 12, pp. 1323-1338, 1990.
[34] J. Li, M.T. Manry, C. Yu, and D.R. Wilson, ?Prototype classifier design with pruning,? International Journal on Artificial Intelligence Tools, vol. 14, no. 1-2, pp. 261-280, 2005.
[35] Y.H. Li, M.J. Pont, and N.B. Jones, ?Improving the performance of radial basis function classifiers in condition monitoring and fault diagnosis applications where 'unknown' faults may occur,? Pattern Recognition Letters, vol. 23, no. 5, pp. 569-577, Mar. 2002.
[36] Y.H. Li, M.J. Pont, N.B. Jones, and J.A. Twiddle, ?Applying MLP and RBF classifiers in embedded condition monitoring and fault diagnosis systems,? Transactions of the Institute of Measurement and Control, vol. 23, no. 5, pp. 315-343, 2001.
[37] T. Lin and H.B. Zha, ?Riemannian manifold learning,? IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 5, pp. 796-809, 2008.
[38] M.T. Lozano, J.M. Sotoca, J.S. Schez, F. Pla, E. Pekalska, and R.P.W. Duin, ?Experimental study on prototype optimisation algorithms for prototype-based classification in vector spaces,? Pattern Recognition, vol. 39, no. 10, pp. 1827-1838, 2006.
[39] A. Lyhyaoui, M. Martinez, I. Mora, M. Vazquez, J.L. Sancho, and A.R. Figueiras-Vidal, ?Sample selection via clustering to construct support vector-like classifiers,? IEEE Transactions on Neural Networks, vol. 10, no. 6, pp. 1474-1481, Nov. 1999.
[40] E. Marchiori, "Hit miss networks with applications to instance selection," Journal of Machine Learning Research, vol. 9, no. 997-1017, 2008.
[41] E. Marchiori, "Class conditional nearest neighbor for large margin instance selection," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 2, pp. 364-370, Feb. 2010.
[42] M. Markou and S. Singh, ?A neural network-based novelty detector for image sequence analysis,? IEEE Transactions On Pattern Analysis And Machine Intelligence, vol. 28, no. 10, pp. 1664-1677, Oct. 2006.
[43] K.G. Mehrotra, C.K. Mohan, and S. Ranka, ?Bounds on the number of samples needed for neural learning,? IEEE Transactions on Neural Networks, vol. 2, no. 6, pp. 548-558, 1991.
[44] N. Mekuz and J.K. Tsotsos, ?Parameterless Isomap with Adaptive Neighborhood Selection,? in K. Franke et al. (Eds.): DAGM 2006, LNCS 4174, pp. 364?373, 2006.
[45] D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz, UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California, Department of Information and Computer Science, 1998.
[46] R. Paredes and E. Vidal, ?Learning prototypes and distances: A prototype reduction technique based on nearest neighbor error minimization,? Pattern Recognition, vol. 39, no. 2, pp. 180-188, 2006.
[47] E. Pekalska, R.P.W. Duin, and P. Paclik, ?Prototype selection for dissimilarity-based classifiers,? Pattern Recognition, vol. 39, no. 2, pp. 189-208, Feb. 2006.
[48] R.T. Peres and C.E. Pedreira, "Generalized risk zone: selecting observations for classification," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 7, pp. 1331-1337,   2009.
[49] D. Prokhorov, ?IJCNN 2001 neural network competition,? Slide presentation in IJCNN?01, Ford Research Laboratory, 2001. http://www.geocities.com/ijcnn/nnc_ijcnn01.pdf.
[50] T. Raicharoen and C. Lursinsap, ?A divide-and-conquer approach to the pairwise opposite class-nearest neighbor (POC-NN) algorithm,? Pattern Recognition Letters, vol. 26, no. 10, pp. 1554-1567, Jul. 2005.
[51] G. Ratsch, T. Onoda, and K.R. Muller, ?Soft margins for AdaBoost,? Machine Learning, vol. 42, no. 2, pp.287-320, 2001.
[52] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, ?Learning representations by back-propagating errors,? Nature, vol. 323, pp. 533-536, Oct. 1986.
[53] H. Samet, "K-nearest neighbor finding using MaxNearestDist," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 2, pp.243-252, Feb. 2008.
[54] O. Samko, A.D. Marshall, and P.L. Rosin ?Selection of the optimal parameter value for the Isomap algorithm,? Pattern Recognition Letters, vol. 27, no. 9, pp. 968-979, Jun. 2006.
[55] J. S. S nchez and A. I. Marqus, ?An LVQ-based adaptive algorithm for learning from very small codebooks,? Neurocomputing, vol. 69, no. 7-9, pp. 922-927, 2006.
[56] J. S. S nchez, ?High training set size reduction by space partitioning and prototype abstraction,? Pattern Recognition, vol. 37, no.  7, pp. 1561-1564, 2004.
[57] J. S. S nchez, F. Pla, and F. J. Ferri, ?Prototype selection for the nearest neighbour rule through proximity graphs,? Pattern Recognition Letters, vol. 18, no. 6, pp. 507-513, 1997.
[58] J.L. Sancho, W.E. Pierson, B. Ulug, A.R. Figueiras-Vidal, and S.C. Ahalt, ?Class separability estimation and incremental learning using boundary methods?, Neurocomputing, vol. 35, pp. 3-26, Nov. 2000.
[59] L. K. Saul and S.T. Roweis, ?Think globally, fit locally: Unsupervised learning of low dimensional manifolds,? Journal of Machine Learning Research, vol. 4, no. 2, pp. 119-155, Feb. 2004.
[60] B. Schlkopf, R.C. Williamson, A.J. Smola, J.S. Taylor, and J.C. Platt, ?Support Vector Method for Novelty Detection,? In Advances in Neural Information Processing Systems, vol.12, pp. 582?588, MIT Press, Cambridge, MA, 2000.
[61] H. Shin and S. Cho, ?Fast pattern selection for support vector classifiers,? Advances in Knowledge Discovery and Data Mining Lecture Notes in Artificial Intelligence, vol. 2637, pp. 376-387, 2003.
[62] H. Shin and S. Cho, ?Neighborhood property-based pattern selection for support vector machines,? Neural Computation, vol. 19, no. 3, pp. 816-855, Mar. 2007.
[63] T. Tambouratzis, ?Counter-clustering for training pattern selection,? Computer Journal, vol. 43, no. 3, pp. 177-190, 2000.
[64] D.M.J. Tax and R.P.W. Duin, ?Support Vector Data Description,? Machine Learning, vol. 54, pp.45?66, 2004.
[65] J.B.Tenenbaum, V. de Silva, and J.C. Langford, ?A global geometric framework for nonlinear dimensionality reduction,? Science, vol. 290, no. 5500, pp. 2319-2323, Dec. 2000.
[66] V.N. Vapnik, The Nature of Statistical Learning Theory. Berlin: Springer-Verlag, 1995.
[67] K.R. Varshney and A.S. Willsky, "Classification Using Geometric Level Sets," Journal of Machine Learning Research, vol. 11, pp. 491-516, Feb. 2010.
[68] C.J. Veenman and M.J.T. Reinders, ?The Nearest Subclass Classifier: A Compromise between the Nearest Mean and Nearest Neighbor Classifier,? IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 9, pp. 1417- 1429, 2005.
[69] G.H Wen, L.J. Jiang, and J. Wen, ?Using locally estimated geodesic distance to optimize neghborhood graph for isometric data embedding,? Pattern Recognition, vol. 41, no. 7,  pp. 2226-2236, Jul. 2008.
[70] D.R. Wilson and T.R. Martinez, ?Reduction techniques for instance-based learning algorithms,? Machine Learning, vol. 38, no. 3, pp. 257-286, Mar. 2000.
[71] H. Zhang and G. Sun, ?Optimal reference subset selection for nearest neighbor classification by tabu search,? Pattern Recognition, vol. 35, no. 7, pp. 1481-1490, 2002.

Yuhua Li received the PhD degree in general engineering from the University of Leicester. He worked at the Manchester Metropolitan University and then the University of Manchester from June 2000 to September 2005 as a Senior Research Fellow and a Research Associate, respectively. Since October 2005, he has been a lecturer at the School of Computing and Intelligent Systems, the University of Ulster. His research interests include pattern recognition, neural networks, knowledge-based systems, signal processing, and condition monitoring and fault diagnosis.

Liam Maguire is a Professor in computational intelligence in the School of Computing and Intelligent Systems, University of Ulster, Magee. He obtained a MEng (Distinction) in 1988 and a PhD in 1991, both in Electrical and Electronic Engineering from the Queens University of Belfast.  He is a member of the IET and a Chartered Engineer. He joined the University of Ulster in 1994 and is a leading member of the Intelligent Systems Research Centre.  He is the author of over 150 research papers. His current research interests are in bio-inspired intelligent systems (such as the development of computationally effective spiking neural networks) and the application of hybrid intelligent techniques.
2   even page	IEEE TRANSACTIONS ON XXXXXXXXXXXXXXXXXXXX,  VOL.  #,  NO.  #,  MMMMMMMM  1996

AUTHOR:  TITLE	odd page    3





IEEE TRANSACTIONS ON PATTERN ANAYLYSIS AND MACHINE INTELLIGENCE,  MANUSCRIPT ID	first page   1



2   even page	IEEE TRANSACTIONS ON PATTERN ANAYLYSIS AND MACHINE INTELLIGENCE,  MANUSCRIPT ID

LI AND MAGUIRE:  SELECTING CRITICAL PATTERNS BASED ON LOCAL GEOMETRICAL AND STATISTICAL INFORMATION	odd page    13
